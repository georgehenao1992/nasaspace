# ====================================================
# OA H√çBRIDO: Gemini + GPT + AVATAR ANIMADO Holografia
# ====================================================
from deepface import DeepFace
import cv2
from collections import deque
import time
import google.generativeai as genai
from openai import OpenAI
from gtts import gTTS
import playsound
import speech_recognition as sr
import os
import threading
import tkinter as tk
from PIL import Image, ImageTk
import queue


# CONFIGURACI√ìN DE APIs
genai.configure(api_key="AIzaSyCY75ZcCZ0_i40qJuE-koUO_Be43Qdk_tA")  # ‚ö†Ô∏è Usa tu clave de Google AI Studio
modelo = genai.GenerativeModel("models/gemini-2.5-flash")
client = OpenAI(api_key="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")


# VARIABLES GLOBALES
hablando = False
frames = []
etiqueta = None  
ventana = None
frame_queue = queue.Queue()  

# CARGAR AVATAR
def cargar_frames(master):
    global frames
    carpeta = "frames"
    for i in range(1, 51):
        ruta = os.path.join(carpeta, f"frame{i}.jpg")
        if os.path.exists(ruta):
            imagen = Image.open(ruta).resize((1600, 1000))
            frames.append(ImageTk.PhotoImage(imagen, master=master))
        else:
            print(f"‚ö†Ô∏è Imagen no encontrada: {ruta}")
    print(f"‚úÖ {len(frames)} im√°genes cargadas para animaci√≥n.")


# ANIMACI√ìN (hilo principal)
def actualizar_animacion():
    global hablando, frames, etiqueta
    if frames:
        if hablando:
            for frame in frames:
                etiqueta.config(image=frame)
                etiqueta.update()
                time.sleep(0.04)
        else:
            etiqueta.config(image=frames[0])
    ventana.after(100, actualizar_animacion)


# VOZ
r = sr.Recognizer()
mic = sr.Microphone()
with mic as source:
    print("üéõÔ∏è Calibrando micr√≥fono...")
    r.adjust_for_ambient_noise(source, duration=1)
print("‚úÖ Micr√≥fono calibrado correctamente.")

def hablar(texto):
    global hablando
    print(f"\nü§ñ OA: {texto}")
    tts = gTTS(text=texto, lang='es')
    archivo = "voz_temp.mp3"
    tts.save(archivo)
    hablando = True
    playsound.playsound(archivo)
    hablando = False
    os.remove(archivo)


# DETECCI√ìN DE EMOCI√ìN (hilo secundario)
def detectar_emocion():
    cap = cv2.VideoCapture(0)
    emotion_queue = deque(maxlen=10)
    rostro_detectado = False
    hablar("Sistema OA activado. Calibrando sensores visuales...")

    start = time.time()
    while not rostro_detectado and (time.time() - start) < 15:
        ret, frame = cap.read()
        if not ret:
            continue
        try:
            deteccion = DeepFace.detectFace(frame, enforce_detection=False)
            if deteccion is not None:
                rostro_detectado = True
                break
        except:
            continue

    if not rostro_detectado:
        hablar("No detect√© rostro, continuar√© con tono neutro.")
        cap.release()
        return "neutral"

    hablar("Rostro detectado. Analizando emoci√≥n...")

    traduccion = {
        "happy": "feliz",
        "sad": "triste",
        "angry": "enojado",
        "neutral": "neutral",
        "fear": "asustado",
        "surprise": "sorprendido",
        "disgust": "disgustado"
    }

    while True:
        ret, frame = cap.read()
        if not ret:
            continue

        try:
            result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)
            dominant = result[0]['dominant_emotion']
        except:
            dominant = "neutral"

        emotion_queue.append(dominant)

        # Enviar frame a la cola para mostrar en el hilo principal
        frame_queue.put(frame)

        # Revisar consistencia de emoci√≥n
        if len(emotion_queue) == emotion_queue.maxlen and len(set(emotion_queue)) == 1:
            emocion_es = traduccion.get(dominant, "neutral")
            hablar(f"Detecto que te sientes {emocion_es}.")
            cap.release()
            return dominant


# RESPUESTA EMOCIONAL
def respuesta_emocional(emocion):
    prompt = f"""
Eres OA, un asistente emp√°tico dentro de una nave espacial.
El rostro del usuario refleja la emoci√≥n: {emocion}.

        "sad": "La persona parece triste. Dile algo amable, motivador y emp√°tico en espa√±ol.",
        "angry": "La persona est√° enojada. Usa un tono calmado y comprensivo para ayudar a tranquilizarla.",
        "happy": "La persona est√° feliz. Refu√©rzalo positivamente con entusiasmo.",
        "neutral": "La persona est√° tranquila. Haz una conversaci√≥n amable y relajada.",
        "fear": "La persona parece asustada. Dale seguridad y calma.",
        "surprise": "La persona est√° sorprendida. Habla con curiosidad positiva."

Responde en m√°ximo 3 l√≠neas, de forma amable y reconfortante.
"""
    try:
        respuesta = modelo.generate_content(prompt)
        return respuesta.text.strip()
    except Exception as e:
        print("‚ö†Ô∏è Error con Gemini:", e)
        return "Hola, noto algo en ti, ¬øc√≥mo te sientes hoy?"


# RESPUESTA GPT
def generar_respuesta(pregunta, emocion):
    contexto = f"""
Eres OA, asistente t√©cnico y emp√°tico de una nave espacial.
El usuario parece {emocion}.
Siempre respondes en espa√±ol, con un tono calmado, t√©cnico y emp√°tico.
Tus respuestas deben:
- Ser cortas (m√°ximo 3 p√°rrafos o 6 l√≠neas).
- Terminar con una sugerencia pr√°ctica.
- No puedo comunicarme con un m√©dico en tierra
- No se pueden realizar acciones de enjuague bucal, est√°s dentro de una nave espacial
- Sugerir conectar al m√©dico en tierra.
- Evitar frases vagas o fuera del entorno espacial.
- Incluir solo recursos disponibles dentro de la nave (no hay tiendas, hospitales ni internet).
- Mostrar seguridad y serenidad, sin generar alarma.
- Si la situaci√≥n es grave, pide calma y describe un procedimiento b√°sico y seguro.

Ejemplo de cierre:
‚ÄúRespira profundo, estamos a salvo y recuerda los protocolos de salud que ya las sabes, puedes contarme c√≥mo es tu dolor‚Äù
M√°ximo 4 l√≠neas por respuesta. No contactes con el exterior ni menciones internet.
"""
    mensajes = [
        {"role": "system", "content": contexto},
        {"role": "user", "content": pregunta}
    ]
    try:
        respuesta = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=mensajes
        )
        return respuesta.choices[0].message.content.strip()
    except Exception as e:
        print("‚ö†Ô∏è Error con GPT:", e)
        return "Tuve una interferencia en el canal, ¬øpuedes repetir?"


# CONVERSACI√ìN
def conversar(emocion):
    hablar("Estoy aqu√≠ para escucharte. Cuando termines, di 'gracias por tu ayuda'.")
    while True:
        try:
            print("üéß Escuchando...")
            audio = r.listen(mic, timeout=6, phrase_time_limit=8)
            texto = r.recognize_google(audio, language="es-ES").lower()
            print(f"üë§ Usuario: {texto}")

            if "gracias por tu ayuda" in texto or "salir" in texto:
                hablar("Ha sido un placer asistirte. Cerrando comunicaci√≥n.")
                break

            respuesta = generar_respuesta(texto, emocion)
            hablar(respuesta)

        except sr.UnknownValueError:
            hablar("No te entend√≠, ¬øpodr√≠as repetirlo?")
        except Exception as e:
            print("‚ö†Ô∏è Error general:", e)
            continue


# MAIN LOOP
def mostrar_frames():
    """Mostrar frames de emoci√≥n desde la cola en el hilo principal"""
    try:
        if not frame_queue.empty():
            frame = frame_queue.get()
            cv2.putText(frame, "OA - An√°lisis emocional", (40, 50),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)
            cv2.imshow("OA - An√°lisis emocional", frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            cv2.destroyAllWindows()
            return
    except:
        pass
    ventana.after(30, mostrar_frames)

def main():
    global ventana, etiqueta
    ventana = tk.Tk()
    ventana.title("OA - Avatar Animado")
    ventana.configure(bg="black")
    ventana.geometry("400x400")

    cargar_frames(ventana)
    etiqueta = tk.Label(ventana, bg="black")
    etiqueta.pack(expand=True)
    ventana.after(100, actualizar_animacion)
    ventana.after(30, mostrar_frames)

    def hilo_logico():
        hablar("Di 'Hola OA' para activar el sistema de tu holograma")
        while True:
            with mic as source:
                print("üéß Esperando activaci√≥n...")
                audio = r.listen(source, timeout=None, phrase_time_limit=2)
                try:
                    texto = r.recognize_google(audio, language="es-ES").lower()
                    print(f"üéôÔ∏è Escuchado: {texto}")
                except:
                    continue

                if "Hola o a" in texto or "hola escucha" in texto or "oa escucha" in texto or "o a escucha" in texto or "hola oa" in texto or "hola" in texto or "hola hola" in texto:
                    hablar("Te escucho, iniciando escaneo emocional.")
                    emocion = detectar_emocion()
                    mensaje_inicial = respuesta_emocional(emocion)
                    hablar(mensaje_inicial)
                    conversar(emocion)

    threading.Thread(target=hilo_logico, daemon=True).start()
    ventana.mainloop()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()